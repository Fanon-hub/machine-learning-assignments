{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 1 - Creating a one-dimensional convolutional layer class with the number of channels limited to 1",
   "id": "665fe972ccdfedd6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.054041Z",
     "start_time": "2024-12-16T09:35:16.022775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleConv1d:\n",
    "    def __init__(self, filter_size, input_size, learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize the SimpleConv1d layer.\n",
    "\n",
    "        :param filter_size: Size of the convolutional filter (F).\n",
    "        :param input_size: Size of the input array (used for weight initialization).\n",
    "        :param learning_rate: Learning rate for weight and bias updates.\n",
    "        \"\"\"\n",
    "        self.filter_size = filter_size\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Xavier initialization for weights and bias\n",
    "        self.weights = np.random.randn(filter_size) * np.sqrt(2.0 / input_size)\n",
    "        self.bias = 0.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform forward propagation.\n",
    "\n",
    "        :param x: Input array (1D).\n",
    "        :return: Output array after convolution.\n",
    "        \"\"\"\n",
    "        self.x = x  # Store input for backpropagation\n",
    "        self.output_size = len(x) - self.filter_size + 1  # Compute output size\n",
    "        self.a = np.zeros(self.output_size)  # Initialize output array\n",
    "\n",
    "        # Convolution operation\n",
    "        for i in range(self.output_size):\n",
    "            self.a[i] = np.sum(self.x[i:i + self.filter_size] * self.weights) + self.bias\n",
    "\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        Perform backward propagation.\n",
    "\n",
    "        :param da: Gradient of the loss with respect to the output (a).\n",
    "        :return: Gradient of the loss with respect to the input (x).\n",
    "        \"\"\"\n",
    "        # Gradients for weights and bias\n",
    "        self.dw = np.zeros_like(self.weights)\n",
    "        for s in range(self.filter_size):\n",
    "            for i in range(self.output_size):\n",
    "                self.dw[s] += da[i] * self.x[i + s]\n",
    "\n",
    "        self.db = np.sum(da)\n",
    "\n",
    "        # Gradient of the loss with respect to the input\n",
    "        dx = np.zeros_like(self.x)\n",
    "        for j in range(len(self.x)):\n",
    "            for s in range(self.filter_size):\n",
    "                if 0 <= j - s < self.output_size:\n",
    "                    dx[j] += da[j - s] * self.weights[s]\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update weights and bias using gradients.\n",
    "        \"\"\"\n",
    "        self.weights -= self.learning_rate * self.dw\n",
    "        self.bias -= self.learning_rate * self.db\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(0)  # For reproducibility\n",
    "\n",
    "    # Initialize SimpleConv1d\n",
    "    filter_size = 3\n",
    "    input_size = 10\n",
    "    learning_rate = 0.01\n",
    "    conv_layer = SimpleConv1d(filter_size, input_size, learning_rate)\n",
    "\n",
    "    # Input array\n",
    "    x = np.random.randn(input_size)\n",
    "\n",
    "    # Forward propagation\n",
    "    output = conv_layer.forward(x)\n",
    "    print(\"Output:\", output)\n",
    "\n",
    "    # Backward propagation (example gradient of the output)\n",
    "    da = np.random.randn(len(output))\n",
    "    dx = conv_layer.backward(da)\n",
    "    print(\"Gradient w.r.t input:\", dx)\n",
    "\n",
    "    # Update weights\n",
    "    conv_layer.update()\n",
    "    print(\"Updated weights:\", conv_layer.weights)\n",
    "    print(\"Updated bias:\", conv_layer.bias)\n"
   ],
   "id": "ed5d7d75138bd25f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [ 1.67430988  1.7143007  -0.66720854  0.67726689  0.04184244  0.05509734\n",
      "  0.98624463  0.70699772]\n",
      "Gradient w.r.t input: [ 0.09599042  0.37194178  0.39592804  1.43268528  0.2515739   0.86423321\n",
      " -0.70757665 -2.02988864 -0.83071411 -1.11745624]\n",
      "Updated weights: [0.77415498 0.21697288 0.46896568]\n",
      "Updated bias: 0.00905884467480576\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 2 - Calculating the output size after 1D convolution",
   "id": "31113e250e2b4aa7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.163258Z",
     "start_time": "2024-12-16T09:35:16.141566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_output_size(N_in, P, F, S):\n",
    "    \"\"\"\n",
    "    Function to calculate the output size of a convolutional layer.\n",
    "\n",
    "    Parameters:\n",
    "    N_in (int): Input size (number of features)\n",
    "    P (int): Padding size\n",
    "    F (int): Filter size\n",
    "    S (int): Stride size\n",
    "\n",
    "    Returns:\n",
    "    int: Output size (number of features)\n",
    "    \"\"\"\n",
    "    N_out = (N_in + 2 * P - F) // S + 1\n",
    "    return N_out\n",
    "\n",
    "# Example\n",
    "N_in = 64  # Input size\n",
    "P = 1      # Padding size\n",
    "F = 3      # Filter size\n",
    "S = 1      # Stride size\n",
    "\n",
    "output_size = calculate_output_size(N_in, P, F, S)\n",
    "output_size\n"
   ],
   "id": "4b12a2b1df1b0a93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 3 - Experiment with 1D convolutional layers on small arrays",
   "id": "b2854a91f8759147"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.192507Z",
     "start_time": "2024-12-16T09:35:16.175248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given input, weights, and bias\n",
    "x = np.array([1, 2, 3, 4])\n",
    "w = np.array([3, 5, 7])\n",
    "b = np.array([1])\n",
    "\n",
    "# Forward propagation\n",
    "# Create an empty array for storing output\n",
    "a = np.empty((2, 3))  # 2 outputs, each of length 3 (filter size)\n",
    "\n",
    "# Indexing to perform the convolution (efficient way)\n",
    "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(int)  # Replace np.int with int\n",
    "a = (x[indexes] * w).sum(axis=1) + b\n",
    "\n",
    "print(\"Output after forward propagation:\")\n",
    "print(a)\n",
    "\n",
    "# Backpropagation\n",
    "delta_a = np.array([10, 20])\n",
    "\n",
    "# Compute the gradients for the bias (delta_b)\n",
    "delta_b = delta_a.sum()\n",
    "\n",
    "# Compute the gradients for the weights (delta_w)\n",
    "delta_w = np.empty_like(w)\n",
    "for i in range(len(w)):\n",
    "    delta_w[i] = (x[indexes[:, i]] * delta_a).sum()\n",
    "\n",
    "# Compute the gradients for the input (delta_x)\n",
    "# Initialize delta_x to zeros\n",
    "delta_x = np.zeros_like(x)\n",
    "\n",
    "# Spread the error back through the input x\n",
    "for i in range(len(delta_a)):\n",
    "    for j in range(len(w)):\n",
    "        delta_x[indexes[i, j]] += delta_a[i] * w[j]\n",
    "\n",
    "print(\"\\nBackpropagation results:\")\n",
    "print(\"delta_b:\", delta_b)\n",
    "print(\"delta_w:\", delta_w)\n",
    "print(\"delta_x:\", delta_x)\n",
    "\n"
   ],
   "id": "4b283dd88cbfa26",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after forward propagation:\n",
      "[35 50]\n",
      "\n",
      "Backpropagation results:\n",
      "delta_b: 30\n",
      "delta_w: [ 50  80 110]\n",
      "delta_x: [ 30 110 170 140]\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 4 - Creating a 1D convolutional layer class with no limit on the number of channels",
   "id": "4c153eb63d35fc0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.261191Z",
     "start_time": "2024-12-16T09:35:16.235554Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, filter_size):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.ones((out_channels, in_channels, filter_size), dtype=np.float64)  # Ensure dtype is float64\n",
    "        self.biases = np.array([i+1 for i in range(out_channels)], dtype=np.float64)  # Ensure dtype is float64\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "        - x: Input array of shape (in_channels, features)\n",
    "\n",
    "        Returns:\n",
    "        - a: Output array after applying the convolution\n",
    "        \"\"\"\n",
    "        # Ensure input x is float64\n",
    "        x = x.astype(np.float64)\n",
    "\n",
    "        # Output shape\n",
    "        out_features = x.shape[1] - self.filter_size + 1  # Feature length after convolution\n",
    "\n",
    "        # Initialize output array\n",
    "        a = np.zeros((self.out_channels, out_features), dtype=np.float64)\n",
    "\n",
    "        # Perform convolution for each output channel\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(out_features):\n",
    "                # Apply filter for this output channel\n",
    "                conv_sum = 0\n",
    "                for k in range(self.in_channels):\n",
    "                    conv_sum += np.sum(x[k, j:j+self.filter_size] * self.weights[i, k])  # Convolution operation\n",
    "                a[i, j] = conv_sum + self.biases[i]  # Add the bias\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, x, delta_a):\n",
    "        \"\"\"\n",
    "        Perform the backpropagation for the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "        - x: Input array of shape (in_channels, features)\n",
    "        - delta_a: Gradient of the loss with respect to the output\n",
    "\n",
    "        Returns:\n",
    "        - delta_w: Gradient of the loss with respect to the weights\n",
    "        - delta_b: Gradient of the loss with respect to the biases\n",
    "        - delta_x: Gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        delta_w = np.zeros_like(self.weights, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.biases, dtype=np.float64)\n",
    "        delta_x = np.zeros_like(x, dtype=np.float64)\n",
    "\n",
    "        # Ensure delta_a is float64\n",
    "        delta_a = delta_a.astype(np.float64)\n",
    "\n",
    "        # Backpropagation for each output channel\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(delta_a.shape[1]):  # For each feature in the output\n",
    "                # Compute the gradient with respect to the bias\n",
    "                delta_b[i] += delta_a[i, j]\n",
    "\n",
    "                # Compute the gradient with respect to the weights\n",
    "                for k in range(self.in_channels):\n",
    "                    delta_w[i, k] += x[k, j:j+self.filter_size] * delta_a[i, j]\n",
    "\n",
    "                # Compute the gradient with respect to the input\n",
    "                for k in range(self.in_channels):\n",
    "                    delta_x[k, j:j+self.filter_size] += self.weights[i, k] * delta_a[i, j]\n",
    "\n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "# Example Usage\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]], dtype=np.float64)  # (2, 4), 2 input channels, 4 features\n",
    "w = np.ones((3, 2, 3), dtype=np.float64)  # (3, 2, 3), 3 output channels, 2 input channels, filter size 3\n",
    "b = np.array([1, 2, 3], dtype=np.float64)  # Bias for each output channel\n",
    "\n",
    "# Create Conv1d layer\n",
    "conv = Conv1d(in_channels=2, out_channels=3, filter_size=3)\n",
    "\n",
    "# Perform forward pass\n",
    "output = conv.forward(x)\n",
    "print(\"Output after forward pass:\")\n",
    "print(output)\n",
    "\n",
    "# Define error from next layer (delta_a)\n",
    "delta_a = np.array([[10, 20], [15, 25], [30, 35]], dtype=np.float64)\n",
    "\n",
    "# Perform backward pass\n",
    "delta_w, delta_b, delta_x = conv.backward(x, delta_a)\n",
    "\n",
    "print(\"\\nGradients after backward pass:\")\n",
    "print(\"delta_w:\", delta_w)\n",
    "print(\"delta_b:\", delta_b)\n",
    "print(\"delta_x:\", delta_x)\n"
   ],
   "id": "21fbe8fb75dde969",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after forward pass:\n",
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "\n",
      "Gradients after backward pass:\n",
      "delta_w: [[[ 50.  80. 110.]\n",
      "  [ 80. 110. 140.]]\n",
      "\n",
      " [[ 65. 105. 145.]\n",
      "  [105. 145. 185.]]\n",
      "\n",
      " [[100. 165. 230.]\n",
      "  [165. 230. 295.]]]\n",
      "delta_b: [30. 40. 65.]\n",
      "delta_x: [[ 55. 135. 135.  80.]\n",
      " [ 55. 135. 135.  80.]]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 5 - (Advanced) Implementing padding",
   "id": "7c43e09b50245ac4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.390506Z",
     "start_time": "2024-12-16T09:35:16.348531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, filter_size, padding=0, padding_type='constant'):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.ones((out_channels, in_channels, filter_size), dtype=np.float64)  # Ensure dtype is float64\n",
    "        self.biases = np.array([i+1 for i in range(out_channels)], dtype=np.float64)  # Ensure dtype is float64\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "        - x: Input array of shape (in_channels, features)\n",
    "\n",
    "        Returns:\n",
    "        - a: Output array after applying the convolution\n",
    "        \"\"\"\n",
    "        # Ensure input x is float64\n",
    "        x = x.astype(np.float64)\n",
    "\n",
    "        # Apply padding to the input if necessary\n",
    "        if self.padding > 0:\n",
    "            if self.padding_type == 'constant':\n",
    "                x = np.pad(x, ((0, 0), (self.padding, self.padding)), mode='constant', constant_values=0)\n",
    "            elif self.padding_type == 'edge':\n",
    "                x = np.pad(x, ((0, 0), (self.padding, self.padding)), mode='edge')\n",
    "\n",
    "        # Output shape (adjusted for padding)\n",
    "        out_features = x.shape[1] - self.filter_size + 1  # Feature length after convolution\n",
    "\n",
    "        # Initialize output array\n",
    "        a = np.zeros((self.out_channels, out_features), dtype=np.float64)\n",
    "\n",
    "        # Perform convolution for each output channel\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(out_features):\n",
    "                # Apply filter for this output channel\n",
    "                conv_sum = 0\n",
    "                for k in range(self.in_channels):\n",
    "                    conv_sum += np.sum(x[k, j:j+self.filter_size] * self.weights[i, k])  # Convolution operation\n",
    "                a[i, j] = conv_sum + self.biases[i]  # Add the bias\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, x, delta_a):\n",
    "        \"\"\"\n",
    "        Perform the backpropagation for the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "        - x: Input array of shape (in_channels, features)\n",
    "        - delta_a: Gradient of the loss with respect to the output\n",
    "\n",
    "        Returns:\n",
    "        - delta_w: Gradient of the loss with respect to the weights\n",
    "        - delta_b: Gradient of the loss with respect to the biases\n",
    "        - delta_x: Gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        delta_w = np.zeros_like(self.weights, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.biases, dtype=np.float64)\n",
    "        delta_x = np.zeros_like(x, dtype=np.float64)\n",
    "\n",
    "        # Ensure delta_a is float64\n",
    "        delta_a = delta_a.astype(np.float64)\n",
    "\n",
    "        # Backpropagation for each output channel\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(delta_a.shape[1]):  # For each feature in the output\n",
    "                # Compute the gradient with respect to the bias\n",
    "                delta_b[i] += delta_a[i, j]\n",
    "\n",
    "                # Compute the gradient with respect to the weights\n",
    "                for k in range(self.in_channels):\n",
    "                    delta_w[i, k] += x[k, j:j+self.filter_size] * delta_a[i, j]\n",
    "\n",
    "                # Compute the gradient with respect to the input\n",
    "                for k in range(self.in_channels):\n",
    "                    delta_x[k, j:j+self.filter_size] += self.weights[i, k] * delta_a[i, j]\n",
    "\n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "# Example Usage\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]], dtype=np.float64)  # (2, 4), 2 input channels, 4 features\n",
    "w = np.ones((3, 2, 3), dtype=np.float64)  # (3, 2, 3), 3 output channels, 2 input channels, filter size 3\n",
    "b = np.array([1, 2, 3], dtype=np.float64)  # Bias for each output channel\n",
    "\n",
    "# Create Conv1d layer with padding\n",
    "conv = Conv1d(in_channels=2, out_channels=3, filter_size=3, padding=1, padding_type='constant')\n",
    "\n",
    "# Perform forward pass\n",
    "output = conv.forward(x)\n",
    "print(\"Output after forward pass:\")\n",
    "print(output)\n",
    "\n",
    "# Define error from next layer (delta_a)\n",
    "delta_a = np.array([[10, 20], [15, 25], [30, 35]], dtype=np.float64)\n",
    "\n",
    "# Perform backward pass\n",
    "delta_w, delta_b, delta_x = conv.backward(x, delta_a)\n",
    "\n",
    "print(\"\\nGradients after backward pass:\")\n",
    "print(\"delta_w:\", delta_w)\n",
    "print(\"delta_b:\", delta_b)\n",
    "print(\"delta_x:\", delta_x)\n"
   ],
   "id": "e3177676526a3504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after forward pass:\n",
      "[[ 9. 16. 22. 17.]\n",
      " [10. 17. 23. 18.]\n",
      " [11. 18. 24. 19.]]\n",
      "\n",
      "Gradients after backward pass:\n",
      "delta_w: [[[ 50.  80. 110.]\n",
      "  [ 80. 110. 140.]]\n",
      "\n",
      " [[ 65. 105. 145.]\n",
      "  [105. 145. 185.]]\n",
      "\n",
      " [[100. 165. 230.]\n",
      "  [165. 230. 295.]]]\n",
      "delta_b: [30. 40. 65.]\n",
      "delta_x: [[ 55. 135. 135.  80.]\n",
      " [ 55. 135. 135.  80.]]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 6 - (Advanced problem) Dealing with mini-batches",
   "id": "7c7efa8a96b23732"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.526037Z",
     "start_time": "2024-12-16T09:35:16.474459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, in_channels, out_channels, filter_size, padding=0, padding_type='constant'):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.filter_size = filter_size\n",
    "        self.padding = padding\n",
    "        self.padding_type = padding_type\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.ones((out_channels, in_channels, filter_size), dtype=np.float64)  # Ensure dtype is float64\n",
    "        self.biases = np.array([i+1 for i in range(out_channels)], dtype=np.float64)  # Ensure dtype is float64\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass through the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "        - x: Input array of shape (batch_size, in_channels, features)\n",
    "\n",
    "        Returns:\n",
    "        - a: Output array after applying the convolution (batch_size, out_channels, features)\n",
    "        \"\"\"\n",
    "        # Ensure input x is float64\n",
    "        x = x.astype(np.float64)\n",
    "\n",
    "        # Apply padding to the input if necessary\n",
    "        if self.padding > 0:\n",
    "            if self.padding_type == 'constant':\n",
    "                x = np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding)), mode='constant', constant_values=0)\n",
    "            elif self.padding_type == 'edge':\n",
    "                x = np.pad(x, ((0, 0), (0, 0), (self.padding, self.padding)), mode='edge')\n",
    "\n",
    "        # Output shape (adjusted for padding)\n",
    "        batch_size, _, in_features = x.shape\n",
    "        out_features = in_features - self.filter_size + 1  # Feature length after convolution\n",
    "\n",
    "        # Initialize output array\n",
    "        a = np.zeros((batch_size, self.out_channels, out_features), dtype=np.float64)\n",
    "\n",
    "        # Perform convolution for each sample in the batch\n",
    "        for b in range(batch_size):\n",
    "            for i in range(self.out_channels):\n",
    "                for j in range(out_features):\n",
    "                    # Apply filter for this output channel and batch sample\n",
    "                    conv_sum = 0\n",
    "                    for k in range(self.in_channels):\n",
    "                        conv_sum += np.sum(x[b, k, j:j+self.filter_size] * self.weights[i, k])  # Convolution operation\n",
    "                    a[b, i, j] = conv_sum + self.biases[i]  # Add the bias\n",
    "\n",
    "        return a\n",
    "\n",
    "    def backward(self, x, delta_a):\n",
    "        \"\"\"\n",
    "        Perform the backpropagation for the convolutional layer.\n",
    "\n",
    "        Args:\n",
    "        - x: Input array of shape (batch_size, in_channels, features)\n",
    "        - delta_a: Gradient of the loss with respect to the output (batch_size, out_channels, features)\n",
    "\n",
    "        Returns:\n",
    "        - delta_w: Gradient of the loss with respect to the weights (out_channels, in_channels, filter_size)\n",
    "        - delta_b: Gradient of the loss with respect to the biases (out_channels,)\n",
    "        - delta_x: Gradient of the loss with respect to the input (batch_size, in_channels, features)\n",
    "        \"\"\"\n",
    "        delta_w = np.zeros_like(self.weights, dtype=np.float64)\n",
    "        delta_b = np.zeros_like(self.biases, dtype=np.float64)\n",
    "        delta_x = np.zeros_like(x, dtype=np.float64)\n",
    "\n",
    "        # Ensure delta_a is float64\n",
    "        delta_a = delta_a.astype(np.float64)\n",
    "\n",
    "        # Backpropagation for each output channel in each sample of the batch\n",
    "        for b in range(x.shape[0]):  # Loop over the batch size\n",
    "            for i in range(self.out_channels):\n",
    "                for j in range(delta_a.shape[2]):  # For each feature in the output\n",
    "                    # Compute the gradient with respect to the bias\n",
    "                    delta_b[i] += delta_a[b, i, j]\n",
    "\n",
    "                    # Compute the gradient with respect to the weights\n",
    "                    for k in range(self.in_channels):\n",
    "                        delta_w[i, k] += x[b, k, j:j+self.filter_size] * delta_a[b, i, j]\n",
    "\n",
    "                    # Compute the gradient with respect to the input\n",
    "                    for k in range(self.in_channels):\n",
    "                        delta_x[b, k, j:j+self.filter_size] += self.weights[i, k] * delta_a[b, i, j]\n",
    "\n",
    "        return delta_w, delta_b, delta_x\n",
    "\n",
    "# Example Usage\n",
    "x = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]],  # (batch_size=2, in_channels=2, features=4)\n",
    "              [[5, 6, 7, 8], [6, 7, 8, 9]]], dtype=np.float64)  # 2 samples in the batch\n",
    "w = np.ones((3, 2, 3), dtype=np.float64)  # (3, 2, 3), 3 output channels, 2 input channels, filter size 3\n",
    "b = np.array([1, 2, 3], dtype=np.float64)  # Bias for each output channel\n",
    "\n",
    "# Create Conv1d layer with padding\n",
    "conv = Conv1d(in_channels=2, out_channels=3, filter_size=3, padding=1, padding_type='constant')\n",
    "\n",
    "# Perform forward pass\n",
    "output = conv.forward(x)\n",
    "print(\"Output after forward pass:\")\n",
    "print(output)\n",
    "\n",
    "# Define error from next layer (delta_a)\n",
    "delta_a = np.array([[[10, 20], [15, 25], [30, 35]],  # Gradient for first sample\n",
    "                    [[5, 10], [15, 20], [25, 30]]], dtype=np.float64)  # Gradient for second sample\n",
    "\n",
    "# Perform backward pass\n",
    "delta_w, delta_b, delta_x = conv.backward(x, delta_a)\n",
    "\n",
    "print(\"\\nGradients after backward pass:\")\n",
    "print(\"delta_w:\", delta_w)\n",
    "print(\"delta_b:\", delta_b)\n",
    "print(\"delta_x:\", delta_x)\n"
   ],
   "id": "d910a6f4ffe99eae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output after forward pass:\n",
      "[[[ 9. 16. 22. 17.]\n",
      "  [10. 17. 23. 18.]\n",
      "  [11. 18. 24. 19.]]\n",
      "\n",
      " [[25. 40. 46. 33.]\n",
      "  [26. 41. 47. 34.]\n",
      "  [27. 42. 48. 35.]]]\n",
      "\n",
      "Gradients after backward pass:\n",
      "delta_w: [[[135. 180. 225.]\n",
      "  [180. 225. 270.]]\n",
      "\n",
      " [[260. 335. 410.]\n",
      "  [335. 410. 485.]]\n",
      "\n",
      " [[405. 525. 645.]\n",
      "  [525. 645. 765.]]]\n",
      "delta_b: [ 45.  75. 120.]\n",
      "delta_x: [[[ 55. 135. 135.  80.]\n",
      "  [ 55. 135. 135.  80.]]\n",
      "\n",
      " [[ 45. 105. 105.  60.]\n",
      "  [ 45. 105. 105.  60.]]]\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 7 - (Advanced Task) Any number of strides",
   "id": "cd4e3b40881dc002"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:35:16.584523Z",
     "start_time": "2024-12-16T09:35:16.543024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Simple Initializer for weights and biases\n",
    "class SimpleInitializer:\n",
    "    def __init__(self, scale=0.01):\n",
    "        self.scale = scale\n",
    "\n",
    "    def W(self, n_out_channels, n_in_channels, filter_size):\n",
    "        return np.random.randn(n_out_channels, n_in_channels, filter_size) * self.scale\n",
    "\n",
    "    def B(self, n_out_channels):\n",
    "        return np.zeros(n_out_channels)\n",
    "\n",
    "# Function to calculate the output size after convolution\n",
    "def output_size_calculation(input_size, filter_size, padding, stride):\n",
    "    return (input_size - filter_size + 2 * padding) // stride + 1\n",
    "\n",
    "class Conv1d:\n",
    "    def __init__(self, b_size, initializer, optimizer, batch_size, n_in_channels=1, n_out_channels=1, padding=0, stride=1):\n",
    "        self.optimizer = optimizer\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.batch_size = batch_size\n",
    "        self.b_size = b_size  # filter size is now stored in b_size\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)  # (n_out_channels, n_in_channels, filter_size)\n",
    "        self.B = initializer.B(n_out_channels)  # (n_out_channels)\n",
    "        self.n_in_channels = n_in_channels\n",
    "        self.n_out_channels = n_out_channels\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Debug output to check the shape of X\n",
    "        print(f\"Input X shape: {X.shape}\")\n",
    "\n",
    "        self.n_samples = X.shape[0]\n",
    "        self.n_in = X.shape[2]\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.padding, self.stride)\n",
    "\n",
    "        # Data after padding\n",
    "        X = self._padding(X)\n",
    "        self.X = X\n",
    "        print(f\"Padded X shape: {self.X.shape}\")  # Shape after padding\n",
    "\n",
    "        # Calculating A (output of the convolution operation)\n",
    "        A = np.zeros((self.n_samples, self.n_out_channels, self.n_out))\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_out_channels):\n",
    "                for k in range(self.n_out):\n",
    "                    start_index = k * self.stride\n",
    "                    A[i, j, k] = np.sum(X[i, :, start_index:start_index + self.b_size] * self.W[j]) + self.B[j]\n",
    "\n",
    "        # Debug output for the final output of the Conv1d layer\n",
    "        print(f\"Conv1d output (A) shape: {A.shape}\")\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.dB = np.zeros_like(self.B)\n",
    "        dX = np.zeros_like(self.X)\n",
    "        for i in range(self.n_samples):\n",
    "            for j in range(self.n_out_channels):\n",
    "                for k in range(self.n_out):\n",
    "                    start_index = k * self.stride\n",
    "                    self.dW[j] += dA[i, j, k] * self.X[i, :, start_index:start_index + self.b_size]\n",
    "                    self.dB[j] += dA[i, j, k]\n",
    "                    dX[i, :, start_index:start_index + self.b_size] += dA[i, j, k] * self.W[j]\n",
    "        self.optimizer.update(self)\n",
    "        dX = dX[:, :, self.padding:self.n_in + self.padding]\n",
    "        return dX\n",
    "\n",
    "    def _padding(self, X):\n",
    "        if self.padding == 0:\n",
    "            return X\n",
    "        return np.pad(X, ((0, 0), (0, 0), (self.padding, self.padding)), 'constant')\n",
    "\n",
    "\n",
    "class DummyOptimizer:\n",
    "    def update(self, conv_layer):\n",
    "        # Dummy optimizer for the sake of example (does not update weights or biases)\n",
    "        print(\"Optimizer update called.\")\n",
    "\n",
    "# Example Usage\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])  # (in_channels=2, input_size=4)\n",
    "x = x.reshape(1, 2, 4)  # Reshaping to (batch_size=1, in_channels=2, input_size=4)\n",
    "\n",
    "conv1d = Conv1d(b_size=3, initializer=SimpleInitializer(0.01), optimizer=DummyOptimizer(), batch_size=1, n_in_channels=2, n_out_channels=3, padding=0, stride=1)\n",
    "\n",
    "output = conv1d.forward(x)\n",
    "\n",
    "print(\"Output after forward pass:\")\n",
    "print(output)\n"
   ],
   "id": "70bf22e0d4741745",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X shape: (1, 2, 4)\n",
      "Padded X shape: (1, 2, 4)\n",
      "Conv1d output (A) shape: (1, 3, 2)\n",
      "Output after forward pass:\n",
      "[[[0.00515442 0.02152479]\n",
      "  [0.0517969  0.07639969]\n",
      "  [0.0231084  0.02183824]]]\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 8 - Learning and estimation",
   "id": "2321964b1f08e0cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T09:39:24.855750Z",
     "start_time": "2024-12-16T09:38:44.547750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "train_labels = to_categorical(train_labels, 10)\n",
    "test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "# Reshape data to (batch_size, channels, input_size) for Conv1d (we flatten the 28x28 image)\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28)  # (batch_size, height, width)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28, 28)  # (batch_size, height, width)\n",
    "\n",
    "# Model definition with Conv1d layers\n",
    "model = models.Sequential()\n",
    "\n",
    "# Use Input layer to define the input shape\n",
    "model.add(layers.Input(shape=(28, 28)))  # Specify the input shape here, instead of in the Conv1D layer\n",
    "\n",
    "# Add Conv1d layers to extract features\n",
    "model.add(layers.Conv1D(32, 3, activation='relu'))  # Conv1d layer with 32 filters\n",
    "model.add(layers.MaxPooling1D(2))  # Max pooling to reduce dimension\n",
    "model.add(layers.Conv1D(64, 3, activation='relu'))  # Another Conv1d layer\n",
    "model.add(layers.MaxPooling1D(2))  # Max pooling to reduce dimension\n",
    "model.add(layers.Conv1D(128, 3, activation='relu'))  # Another Conv1d layer\n",
    "model.add(layers.MaxPooling1D(2))  # Max pooling to reduce dimension\n",
    "\n",
    "# Flatten the output to feed into the fully connected layer\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Output layer (fully connected)\n",
    "model.add(layers.Dense(10, activation='softmax'))  # 10 output units for 10 classes (MNIST)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc}\")\n"
   ],
   "id": "ad94a4b4783adafc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 8ms/step - accuracy: 0.8173 - loss: 0.6109 - val_accuracy: 0.9649 - val_loss: 0.1131\n",
      "Epoch 2/5\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 8ms/step - accuracy: 0.9675 - loss: 0.1102 - val_accuracy: 0.9756 - val_loss: 0.0775\n",
      "Epoch 3/5\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 8ms/step - accuracy: 0.9749 - loss: 0.0802 - val_accuracy: 0.9786 - val_loss: 0.0715\n",
      "Epoch 4/5\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 7ms/step - accuracy: 0.9802 - loss: 0.0626 - val_accuracy: 0.9801 - val_loss: 0.0635\n",
      "Epoch 5/5\n",
      "\u001B[1m938/938\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 8ms/step - accuracy: 0.9851 - loss: 0.0484 - val_accuracy: 0.9815 - val_loss: 0.0606\n",
      "313/313 - 1s - 3ms/step - accuracy: 0.9815 - loss: 0.0606\n",
      "Test accuracy: 0.9815000295639038\n"
     ]
    }
   ],
   "execution_count": 42
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
