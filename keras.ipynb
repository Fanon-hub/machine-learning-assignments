{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problem 1 - Share and run the official tutorial model\n",
    "\n",
    "Tutorial for beginners"
   ],
   "id": "13c4113cc3a4b830"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-17T12:28:21.271823Z",
     "start_time": "2024-12-17T12:28:21.224951Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:29:18.130661Z",
     "start_time": "2024-12-17T12:28:21.287447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Keep softmax here\n",
    "])\n",
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions\n",
    "\n",
    "tf.nn.softmax(predictions).numpy()\n",
    "# Defining a loss function for training using `losses.SparseCategoricalCrossentropy`:\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "loss_fn(y_train[:1], predictions).numpy()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',  # from_logits=False is the default\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "# training and evaluating the model\n",
    "model.fit(x_train, y_train, epochs=5)\n"
   ],
   "id": "e15aa69322e145aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 6ms/step - accuracy: 0.8741 - loss: 0.4314\n",
      "Epoch 2/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 5ms/step - accuracy: 0.9639 - loss: 0.1222\n",
      "Epoch 3/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 5ms/step - accuracy: 0.9769 - loss: 0.0761\n",
      "Epoch 4/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 5ms/step - accuracy: 0.9830 - loss: 0.0566\n",
      "Epoch 5/5\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 5ms/step - accuracy: 0.9878 - loss: 0.0418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f486c1aef0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluating the model\n",
   "id": "6c0364a57ae7f69c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:29:19.833683Z",
     "start_time": "2024-12-17T12:29:18.214521Z"
    }
   },
   "cell_type": "code",
   "source": "model.evaluate(x_test,  y_test, verbose=2)",
   "id": "f786752d2991bb43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - 4ms/step - accuracy: 0.9770 - loss: 0.0783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07829247415065765, 0.9769999980926514]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Returning the probability of the model",
   "id": "74bbcd16ff42be53"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:29:20.102883Z",
     "start_time": "2024-12-17T12:29:20.012904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "\n",
    "probability_model(x_test[:5])"
   ],
   "id": "a36b1fb30a380339",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[0.0853372 , 0.08533719, 0.08533791, 0.08533932, 0.08533719,\n",
       "        0.08533719, 0.08533719, 0.23196223, 0.08533728, 0.08533733],\n",
       "       [0.08533723, 0.08533909, 0.23196174, 0.08533803, 0.08533722,\n",
       "        0.08533777, 0.08533722, 0.08533722, 0.08533723, 0.08533722],\n",
       "       [0.08537082, 0.23144211, 0.08543362, 0.08537189, 0.08540295,\n",
       "        0.08537439, 0.08537769, 0.08545222, 0.08540409, 0.08537026],\n",
       "       [0.2319693 , 0.08533674, 0.08533674, 0.08533674, 0.08533674,\n",
       "        0.08533674, 0.08533674, 0.08533674, 0.08533674, 0.08533674],\n",
       "       [0.08535012, 0.08535007, 0.0853501 , 0.08535007, 0.23175827,\n",
       "        0.08535009, 0.08535011, 0.08535056, 0.08535021, 0.08544043]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TensorFlow 2 quickstart for experts\n",
   "id": "30612ae95b8f80e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:29:21.089097Z",
     "start_time": "2024-12-17T12:29:20.126996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing Tensorflow into the program\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# loading and preparing the mnist dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
    "\n",
    "# Using tf.data to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# Building the tf.keras model using the Keras model subclassing API:\n",
    "class MyModel(Model):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()"
   ],
   "id": "f8a7666a5878d172",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Choose an optimizer and loss function for training:",
   "id": "dd4de663138f5537"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:29:30.605632Z",
     "start_time": "2024-12-17T12:29:30.551864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose an optimizer and loss function for training:\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Selecting metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ],
   "id": "452e0e046a069d2b",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using tf.GradientTape to train the model:",
   "id": "c451784698b9f0b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:42.712244Z",
     "start_time": "2024-12-17T12:29:31.069747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using tf.GradientTape to train the model:\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # training=True is only needed if there are layers with different\n",
    "    # behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "\n",
    "# testing the model\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  # training=False is only needed if there are layers with different\n",
    "  # behavior during training versus inference (e.g. Dropout).\n",
    "  predictions = model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  # Reset the metrics at the start of the next epoch\n",
    "  train_loss.reset_state()\n",
    "  train_accuracy.reset_state()\n",
    "  test_loss.reset_state()\n",
    "  test_accuracy.reset_state()\n",
    "\n",
    "  for images, labels in train_ds:\n",
    "    train_step(images, labels)\n",
    "\n",
    "  for test_images, test_labels in test_ds:\n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "  print(\n",
    "    f'Epoch {epoch + 1}, '\n",
    "    f'Loss: {train_loss.result():0.2f}, '\n",
    "    f'Accuracy: {train_accuracy.result() * 100:0.2f}, '\n",
    "    f'Test Loss: {test_loss.result():0.2f}, '\n",
    "    f'Test Accuracy: {test_accuracy.result() * 100:0.2f}'\n",
    "  )"
   ],
   "id": "6a3dc2f0706802f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.14, Accuracy: 95.83, Test Loss: 0.06, Test Accuracy: 97.98\n",
      "Epoch 2, Loss: 0.04, Accuracy: 98.62, Test Loss: 0.05, Test Accuracy: 98.30\n",
      "Epoch 3, Loss: 0.02, Accuracy: 99.28, Test Loss: 0.05, Test Accuracy: 98.44\n",
      "Epoch 4, Loss: 0.01, Accuracy: 99.54, Test Loss: 0.06, Test Accuracy: 98.43\n",
      "Epoch 5, Loss: 0.01, Accuracy: 99.66, Test Loss: 0.06, Test Accuracy: 98.30\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 2 - (Advanced problem) Implement various methods",
   "id": "febe018753eb142f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:43.047749Z",
     "start_time": "2024-12-17T12:37:42.918133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Loading the iris dataset from scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Storing the iris dataset in a variable\n",
    "data = load_iris()\n",
    "\n",
    "# Preparing lists for row and column indices\n",
    "iris_data_columns=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]\n",
    "iris_target_columns=['Species',]\n",
    "# Converting to pandas DataFrame\n",
    "X_data = pd.DataFrame(data=data['data'], columns=iris_data_columns)\n",
    "y_data = pd.DataFrame(data=data['target'], columns=iris_target_columns)\n",
    "\n",
    "y_data=y_data.replace({0: 'Iris-setosa', 1: 'Iris-versicolor', 2:'Iris-virginica'})\n",
    "\n",
    "# Combining X and y along the column axis\n",
    "df = pd.concat([X_data, y_data], axis=1)\n",
    "display(df)"
   ],
   "id": "bf17ef424154037d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm         Species\n",
       "0              5.1           3.5            1.4           0.2     Iris-setosa\n",
       "1              4.9           3.0            1.4           0.2     Iris-setosa\n",
       "2              4.7           3.2            1.3           0.2     Iris-setosa\n",
       "3              4.6           3.1            1.5           0.2     Iris-setosa\n",
       "4              5.0           3.6            1.4           0.2     Iris-setosa\n",
       "..             ...           ...            ...           ...             ...\n",
       "145            6.7           3.0            5.2           2.3  Iris-virginica\n",
       "146            6.3           2.5            5.0           1.9  Iris-virginica\n",
       "147            6.5           3.0            5.2           2.0  Iris-virginica\n",
       "148            6.2           3.4            5.4           2.3  Iris-virginica\n",
       "149            5.9           3.0            5.1           1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:49.407402Z",
     "start_time": "2024-12-17T12:37:43.872840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Using a neural network implemented in TensorFlow to perform binary classification on the Iris dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Loading the dataset\n",
    "# Uncomment and specify your dataset path if needed\n",
    "# dataset_path = \"Iris.csv\"\n",
    "# df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Filtering and preparing the dataset\n",
    "df_q2 = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
    "y = df_q2[\"Species\"].copy()\n",
    "X = df_q2.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
    "\n",
    "# Converting labels to numbers\n",
    "y = np.where(y == \"Iris-versicolor\", 0, 1).astype(int).reshape(-1, 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "class GetMiniBatch:\n",
    "    def __init__(self, X, y, batch_size=10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = int(np.ceil(X.shape[0] / self.batch_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        p0 = item * self.batch_size\n",
    "        p1 = item * self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter * self.batch_size\n",
    "        p1 = self._counter * self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]\n",
    "\n",
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# Placeholders\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Mini-batch iterator for training\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
    "    return layer_output\n",
    "\n",
    "# Network\n",
    "logits = example_net(X)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Metrics\n",
    "predictions = tf.sigmoid(logits)\n",
    "correct_pred = tf.equal(tf.round(predictions), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for mini_batch_x, mini_batch_y in get_mini_batch_train:\n",
    "            _, loss, acc = sess.run([train_op, loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= len(get_mini_batch_train)\n",
    "        total_acc /= len(get_mini_batch_train)\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {total_acc:.3f}, Val Acc: {val_acc:.3f}\")\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(f\"Test Accuracy: {test_acc:.3f}\")\n"
   ],
   "id": "957668aa6787b7eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9284\\904206475.py:6: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9284\\904206475.py:94: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "Epoch 1, Loss: 34.0635, Val Loss: 9.9256, Acc: 0.421, Val Acc: 0.438\n",
      "Epoch 2, Loss: 3.9368, Val Loss: 9.3914, Acc: 0.757, Val Acc: 0.500\n",
      "Epoch 3, Loss: 5.1304, Val Loss: 8.7450, Acc: 0.736, Val Acc: 0.688\n",
      "Epoch 4, Loss: 6.2432, Val Loss: 2.2494, Acc: 0.657, Val Acc: 0.750\n",
      "Epoch 5, Loss: 0.8559, Val Loss: 1.5456, Acc: 0.900, Val Acc: 0.750\n",
      "Epoch 6, Loss: 1.4737, Val Loss: 3.2962, Acc: 0.843, Val Acc: 0.812\n",
      "Epoch 7, Loss: 2.1738, Val Loss: 1.3548, Acc: 0.814, Val Acc: 0.875\n",
      "Epoch 8, Loss: 0.7770, Val Loss: 0.9560, Acc: 0.929, Val Acc: 0.750\n",
      "Epoch 9, Loss: 1.0334, Val Loss: 5.3889, Acc: 0.886, Val Acc: 0.750\n",
      "Epoch 10, Loss: 2.7836, Val Loss: 1.0290, Acc: 0.786, Val Acc: 0.812\n",
      "Test Accuracy: 0.850\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create an Iris model using all three types of objective variables Create a model that can classify all three types included in the objective variable Species in train.csv of the Iris dataset. Iris Species Consider the difference between two-class classification and three-class or more classification. Please refer to the official documentation to find out how it can be rewritten in TensorFlow. 《Hint》 The following two sections are processes specific to two-class classification.",
   "id": "a94a521974e6547d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:51.409477Z",
     "start_time": "2024-12-17T12:37:49.594302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "y = df[\"Species\"]\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "\n",
    "# one-hot-enconding\n",
    "y_one = pd.get_dummies(y)\n",
    "\n",
    "y = np.array(y_one)\n",
    "X = np.array(X)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Further split into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Hyperparameter settings\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "num_epochs = 10\n",
    "n_hidden1 = 10\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 3\n",
    "\n",
    "# CalculationDecide the shape of the arguments to be passed to the graph.\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Mini-batch iterator for training\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net_iris(x):\n",
    "    \"\"\"\n",
    "    Simple 3-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaration of weights and biases\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']  # tf.add and + are equivalent\n",
    "    return layer_output\n",
    "\n",
    "# Reading the network structure\n",
    "logits = example_net_iris(X)\n",
    "\n",
    "# Objective function (modified for multi-class classification using softmax)\n",
    "# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# Optimization method\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Predicted results (using argmax for multi-class classification)\n",
    "# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(tf.nn.softmax(logits), 1))  # axis = 1\n",
    "\n",
    "# Metric calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Variable initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Running the computation graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  # Variable initialization\n",
    "    for epoch in range(num_epochs):\n",
    "        # Looping through each epoch, np.ceil rounds up and down (flooring)\n",
    "        total_batch = np.ceil(X_train.shape[0] / batch_size).astype(int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Looping through each mini-batch\n",
    "            # train_op = AdamOptimizer, gradient descent using the Adam algorithm\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ],
   "id": "d8cd5d4be09e9c1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss : 34.2376, val_loss : 40.7104, acc : 0.000, val_acc : 0.292\n",
      "Epoch 1, loss : 7.1858, val_loss : 14.1717, acc : 0.667, val_acc : 0.500\n",
      "Epoch 2, loss : 5.5380, val_loss : 5.7234, acc : 0.667, val_acc : 0.625\n",
      "Epoch 3, loss : 3.5417, val_loss : 3.4755, acc : 0.667, val_acc : 0.625\n",
      "Epoch 4, loss : 0.1669, val_loss : 0.2464, acc : 1.000, val_acc : 0.875\n",
      "Epoch 5, loss : 0.0333, val_loss : 0.5458, acc : 1.000, val_acc : 0.875\n",
      "Epoch 6, loss : 0.0512, val_loss : 0.2946, acc : 1.000, val_acc : 0.833\n",
      "Epoch 7, loss : 0.0210, val_loss : 0.1416, acc : 1.000, val_acc : 0.917\n",
      "Epoch 8, loss : 0.0105, val_loss : 0.2605, acc : 1.000, val_acc : 0.833\n",
      "Epoch 9, loss : 0.0265, val_loss : 0.5400, acc : 1.000, val_acc : 0.792\n",
      "test_acc : 0.933\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a model for House Prices Create a model using House Prices, a dataset for regression problems. House Prices: Advanced Regression Techniques Download train.csv from this file and use SalePrice as the objective variable, and GrlivArea and YearBuilt as explanatory variables. You can add more explanatory variables. Please take into consideration the difference between classification problems and regression problems. Since this is a regression problem, use mean squared error as the objective function (loss function). Also, delete Accuracy.",
   "id": "fa50f8f4420822d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:54.128752Z",
     "start_time": "2024-12-17T12:37:51.536530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loading the dataset\n",
    "df_house = pd.read_csv('train.csv')\n",
    "\n",
    "# Extracting conditions from the DataFrame\n",
    "X = np.array(df_house[['GrLivArea','YearBuilt']])\n",
    "y = np.array(df_house['SalePrice'])\n",
    "\n",
    "# Log transformation\n",
    "X = np.log(np.array(X))\n",
    "y = np.log(np.array(y))\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Further splitting into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 1\n",
    "\n",
    "# Deciding the shape of the arguments to pass to the computation graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Mini-batch iterator for training\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    Simple 3-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaration of weights and biases\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']  # tf.add and + are equivalent\n",
    "    return layer_output\n",
    "\n",
    "# Loading the network structure\n",
    "logits = example_net(X)\n",
    "\n",
    "# Objective function\n",
    "loss_op = tf.losses.mean_squared_error(Y, logits)\n",
    "\n",
    "# Optimization method\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Variable initialization\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Running the computation graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  # Variable initialization\n",
    "    for epoch in range(num_epochs):\n",
    "        # Looping through each epoch, np.ceil rounds up and down (flooring)\n",
    "        total_batch = np.ceil(X_train.shape[0] / batch_size).astype(int)\n",
    "        total_loss = 0\n",
    "        # total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Looping through each mini-batch\n",
    "            # train_op = AdamOptimizer, gradient descent using the Adam algorithm\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "        total_loss /= n_samples\n",
    "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {}, val_loss : {}\".format(epoch, loss, val_loss))\n",
    "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
    "    print(\"test_loss : {:.3f}\".format(test_loss))\n"
   ],
   "id": "cd89aff6fa306ec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_9284\\768709374.py:61: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
      "\n",
      "Epoch 0, loss : 792.2412109375, val_loss : 847.134521484375\n",
      "Epoch 1, loss : 9.81013298034668, val_loss : 9.77377986907959\n",
      "Epoch 2, loss : 15.578558921813965, val_loss : 12.64223575592041\n",
      "Epoch 3, loss : 9.327398300170898, val_loss : 8.554512977600098\n",
      "Epoch 4, loss : 7.839667797088623, val_loss : 7.574936866760254\n",
      "Epoch 5, loss : 6.951971530914307, val_loss : 6.767199516296387\n",
      "Epoch 6, loss : 6.126048564910889, val_loss : 5.960129261016846\n",
      "Epoch 7, loss : 5.299201488494873, val_loss : 5.16073751449585\n",
      "Epoch 8, loss : 4.49656867980957, val_loss : 4.382074356079102\n",
      "Epoch 9, loss : 3.7303855419158936, val_loss : 3.6365530490875244\n",
      "test_loss : 3.763\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a MNIST model Create a model to classify MNIST, which was used in the Scratch Neural Network. This is similar to the previous Iris model in that it classifies three or more classes. The difference is that the input is an image. Aim to reproduce the model implemented in Scratch. Prepare the dataset",
   "id": "ca2d544c78ba7576"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:37:59.068641Z",
     "start_time": "2024-12-17T12:37:54.305651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"MNIST shape\")\n",
    "print(X_train.shape)  # (48000, 784)\n",
    "print(X_test.shape)  # (12000, 784)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "# Reshaping the data\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "# Converting to float and normalizing\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Shape after normalization\")\n",
    "print(X_train.shape)  # (48000, 784)\n",
    "print(X_test.shape)  # (12000, 784)\n",
    "\n",
    "# One-hot encoding the labels\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "print(\"Shape after one-hot encoding\")\n",
    "print(y_train.shape)  # (60000,)\n",
    "print(y_train_one_hot.shape)  # (60000, 10)\n",
    "\n",
    "# Splitting the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "print(\"Shape after train_test_split\")\n",
    "print(X_train.shape)  # (48000, 784)\n",
    "print(X_val.shape)  # (12000, 784)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "\n"
   ],
   "id": "703d020700659f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST shape\n",
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n",
      "(10000,)\n",
      "Shape after normalization\n",
      "(60000, 784)\n",
      "(10000, 784)\n",
      "Shape after one-hot encoding\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "Shape after train_test_split\n",
      "(48000, 784)\n",
      "(12000, 784)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:39.380701Z",
     "start_time": "2024-12-17T12:37:59.695736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 100\n",
    "n_input = X_train.shape[1]\n",
    "n_samples = X_train.shape[0]\n",
    "n_classes = 10\n",
    "\n",
    "# Define the shape of the arguments to pass to the computation graph\n",
    "X = tf.placeholder(\"float\", [None, n_input])\n",
    "Y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Mini-batch iterator for training\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
    "\n",
    "def example_net(x):\n",
    "    \"\"\"\n",
    "    Simple 3-layer neural network\n",
    "    \"\"\"\n",
    "    # Declaration of weights and biases\n",
    "    weights = {\n",
    "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
    "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
    "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
    "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']  # tf.add and + are equivalent\n",
    "    return layer_output\n",
    "\n",
    "# Loading the network structure\n",
    "logits = example_net(X)\n",
    "\n",
    "# Objective function (changed to Softmax)\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n",
    "\n",
    "# Optimization method (changed to AdaGrad)\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)  # Alternative: Adam optimizer\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Estimation results (changed to use argmax)\n",
    "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1))  # axis = 1\n",
    "\n",
    "# Metric calculation\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialization of variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Running the computation graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)  # Initializing variables\n",
    "    for epoch in range(num_epochs):\n",
    "        # Looping through each epoch np.ceil = rounding up and down (flooring)\n",
    "        total_batch = np.ceil(X_train.shape[0] / batch_size).astype(int)\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
    "            # Looping through each mini-batch\n",
    "            # train_op = AdamOptimizer, gradient descent using the Adam algorithm\n",
    "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "        total_loss /= n_samples\n",
    "        total_acc /= n_samples\n",
    "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
    "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n",
    "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
    "    print(\"test_acc : {:.3f}\".format(test_acc))\n"
   ],
   "id": "aff11189ff454994",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:138: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Epoch 0, loss : 31.1209, val_loss : 22.9930, acc : 0.656, val_acc : 0.648\n",
      "Epoch 1, loss : 25.3262, val_loss : 16.7526, acc : 0.688, val_acc : 0.689\n",
      "Epoch 2, loss : 21.8767, val_loss : 13.9615, acc : 0.688, val_acc : 0.711\n",
      "Epoch 3, loss : 20.1204, val_loss : 12.2594, acc : 0.688, val_acc : 0.721\n",
      "Epoch 4, loss : 19.1155, val_loss : 11.1055, acc : 0.688, val_acc : 0.729\n",
      "Epoch 5, loss : 18.3071, val_loss : 10.2571, acc : 0.656, val_acc : 0.734\n",
      "Epoch 6, loss : 17.6142, val_loss : 9.5960, acc : 0.688, val_acc : 0.740\n",
      "Epoch 7, loss : 16.9320, val_loss : 9.0705, acc : 0.656, val_acc : 0.744\n",
      "Epoch 8, loss : 16.2155, val_loss : 8.6284, acc : 0.656, val_acc : 0.748\n",
      "Epoch 9, loss : 15.6509, val_loss : 8.2502, acc : 0.656, val_acc : 0.752\n",
      "test_acc : 0.763\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "CONVERTING THE ABOVE DATA TO KERAS\n",
    "\n",
    "Iris (binary classification of Iris-versicolor and Iris-virginica only)"
   ],
   "id": "52f34a917688be6b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.043288Z",
     "start_time": "2024-12-17T12:38:39.380701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Debug mode for tf.data (only works if eager execution is enabled, which it is by default in TF 2.x)\n",
    "try:\n",
    "    tf.data.experimental.enable_debug_mode()\n",
    "except ValueError:\n",
    "    print(\"Debug mode cannot be enabled. Ensure TensorFlow is running with eager execution.\")\n",
    "\n",
    "# Load the Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Filter only classes 1 and 2\n",
    "binary_indices = (y == 1) | (y == 2)\n",
    "X_binary = X[binary_indices]\n",
    "y_binary = y[binary_indices] - 1  # Convert labels to 0 and 1\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Explicit input layer\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Output for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "id": "5b942b5a07a20b8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug mode cannot be enabled. Ensure TensorFlow is running with eager execution.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 43\u001B[0m\n\u001B[0;32m     40\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary_crossentropy\u001B[39m\u001B[38;5;124m'\u001B[39m, metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     42\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 43\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mX_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Evaluate the model\u001B[39;00m\n\u001B[0;32m     46\u001B[0m loss, accuracy \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate(X_test, y_test)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:503\u001B[0m, in \u001B[0;36mDatasetV2.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m iterator_ops\u001B[38;5;241m.\u001B[39mOwnedIterator(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 503\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.Dataset` only supports Python-style \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    504\u001B[0m                      \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miteration in eager mode or within tf.function.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Corrected Iris Multi-Class Classification Code",
   "id": "7ef12efecd83843c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.065430800Z",
     "start_time": "2024-12-17T11:31:48.164894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Enable debug mode for tf.data functions\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# One-hot encode the target\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model with explicit Input layer\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Explicit Input layer\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(3, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=8)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "id": "bb62959b26f03c9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 38ms/step - accuracy: 0.4406 - loss: 0.8730 - val_accuracy: 0.5333 - val_loss: 0.8531\n",
      "Epoch 2/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.6454 - loss: 0.7832 - val_accuracy: 0.6333 - val_loss: 0.8089\n",
      "Epoch 3/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.6283 - loss: 0.8026 - val_accuracy: 0.6333 - val_loss: 0.7704\n",
      "Epoch 4/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6365 - loss: 0.7600 - val_accuracy: 0.6333 - val_loss: 0.7355\n",
      "Epoch 5/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.6441 - loss: 0.7063 - val_accuracy: 0.6333 - val_loss: 0.7040\n",
      "Epoch 6/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step - accuracy: 0.5911 - loss: 0.6936 - val_accuracy: 0.6333 - val_loss: 0.6730\n",
      "Epoch 7/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.6868 - loss: 0.6273 - val_accuracy: 0.6333 - val_loss: 0.6409\n",
      "Epoch 8/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step - accuracy: 0.7038 - loss: 0.6281 - val_accuracy: 0.6333 - val_loss: 0.6098\n",
      "Epoch 9/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.6520 - loss: 0.5759 - val_accuracy: 0.6333 - val_loss: 0.5782\n",
      "Epoch 10/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step - accuracy: 0.6040 - loss: 0.6174 - val_accuracy: 0.7667 - val_loss: 0.5453\n",
      "Epoch 11/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.7487 - loss: 0.5154 - val_accuracy: 0.9667 - val_loss: 0.5027\n",
      "Epoch 12/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step - accuracy: 0.8526 - loss: 0.4805 - val_accuracy: 0.9667 - val_loss: 0.4618\n",
      "Epoch 13/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 37ms/step - accuracy: 0.7934 - loss: 0.4965 - val_accuracy: 0.9333 - val_loss: 0.4234\n",
      "Epoch 14/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step - accuracy: 0.8327 - loss: 0.4648 - val_accuracy: 0.9000 - val_loss: 0.3891\n",
      "Epoch 15/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.9031 - loss: 0.3687 - val_accuracy: 0.9333 - val_loss: 0.3548\n",
      "Epoch 16/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 0.8609 - loss: 0.3513 - val_accuracy: 0.9333 - val_loss: 0.3311\n",
      "Epoch 17/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 0.8326 - loss: 0.3883 - val_accuracy: 0.9333 - val_loss: 0.3079\n",
      "Epoch 18/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.8651 - loss: 0.3308 - val_accuracy: 0.9333 - val_loss: 0.2864\n",
      "Epoch 19/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 0.8527 - loss: 0.3596 - val_accuracy: 0.9333 - val_loss: 0.2737\n",
      "Epoch 20/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 18ms/step - accuracy: 0.8354 - loss: 0.3473 - val_accuracy: 0.9333 - val_loss: 0.2615\n",
      "Epoch 21/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.8847 - loss: 0.3244 - val_accuracy: 0.9333 - val_loss: 0.2466\n",
      "Epoch 22/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.9021 - loss: 0.2812 - val_accuracy: 0.9333 - val_loss: 0.2364\n",
      "Epoch 23/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 17ms/step - accuracy: 0.8536 - loss: 0.3309 - val_accuracy: 0.9333 - val_loss: 0.2278\n",
      "Epoch 24/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.8632 - loss: 0.3134 - val_accuracy: 0.9333 - val_loss: 0.2198\n",
      "Epoch 25/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.8555 - loss: 0.3151 - val_accuracy: 0.9333 - val_loss: 0.2121\n",
      "Epoch 26/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9242 - loss: 0.2344 - val_accuracy: 0.9667 - val_loss: 0.2033\n",
      "Epoch 27/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.8965 - loss: 0.2697 - val_accuracy: 0.9667 - val_loss: 0.1972\n",
      "Epoch 28/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.8784 - loss: 0.2751 - val_accuracy: 0.9667 - val_loss: 0.1880\n",
      "Epoch 29/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9228 - loss: 0.2587 - val_accuracy: 0.9667 - val_loss: 0.1796\n",
      "Epoch 30/30\n",
      "\u001B[1m15/15\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9125 - loss: 0.2294 - val_accuracy: 0.9667 - val_loss: 0.1719\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 91ms/step - accuracy: 0.9667 - loss: 0.1719\n",
      "Test Loss: 0.1719, Test Accuracy: 0.9667\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Corrected House Prices Regression Code",
   "id": "b7a0aea321f6bc6e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.065430800Z",
     "start_time": "2024-12-17T11:32:39.311480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Enable debug mode for tf.data functions\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "# Load the House Prices dataset (California Housing)\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build the model with explicit Input layer\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Explicit Input layer\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Single neuron for regression output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n"
   ],
   "id": "6342111bb33c16b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 8ms/step - loss: 2.2167 - mae: 0.9414 - val_loss: 0.4484 - val_mae: 0.4746\n",
      "Epoch 2/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 10ms/step - loss: 0.4249 - mae: 0.4650 - val_loss: 0.3948 - val_mae: 0.4520\n",
      "Epoch 3/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 10ms/step - loss: 0.3797 - mae: 0.4427 - val_loss: 0.4054 - val_mae: 0.4443\n",
      "Epoch 4/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 17ms/step - loss: 0.3680 - mae: 0.4286 - val_loss: 0.3646 - val_mae: 0.4354\n",
      "Epoch 5/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.3479 - mae: 0.4170 - val_loss: 0.3459 - val_mae: 0.4196\n",
      "Epoch 6/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 7ms/step - loss: 0.3402 - mae: 0.4058 - val_loss: 0.3388 - val_mae: 0.4068\n",
      "Epoch 7/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.4002 - mae: 0.4024 - val_loss: 0.3279 - val_mae: 0.3967\n",
      "Epoch 8/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.3323 - mae: 0.4008 - val_loss: 0.3235 - val_mae: 0.3964\n",
      "Epoch 9/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.3113 - mae: 0.3907 - val_loss: 0.3182 - val_mae: 0.3935\n",
      "Epoch 10/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.3346 - mae: 0.3907 - val_loss: 0.3096 - val_mae: 0.3860\n",
      "Epoch 11/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2953 - mae: 0.3764 - val_loss: 0.3355 - val_mae: 0.3886\n",
      "Epoch 12/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.3004 - mae: 0.3793 - val_loss: 0.3264 - val_mae: 0.3903\n",
      "Epoch 13/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 5ms/step - loss: 0.3103 - mae: 0.3825 - val_loss: 0.3146 - val_mae: 0.3821\n",
      "Epoch 14/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.3033 - mae: 0.3807 - val_loss: 0.3159 - val_mae: 0.3798\n",
      "Epoch 15/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.4942 - mae: 0.3862 - val_loss: 0.3047 - val_mae: 0.3778\n",
      "Epoch 16/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.2836 - mae: 0.3719 - val_loss: 0.3086 - val_mae: 0.3938\n",
      "Epoch 17/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.2760 - mae: 0.3653 - val_loss: 0.3108 - val_mae: 0.3866\n",
      "Epoch 18/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2914 - mae: 0.3708 - val_loss: 0.3045 - val_mae: 0.3836\n",
      "Epoch 19/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2722 - mae: 0.3629 - val_loss: 0.3022 - val_mae: 0.3767\n",
      "Epoch 20/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2789 - mae: 0.3650 - val_loss: 0.2957 - val_mae: 0.3777\n",
      "Epoch 21/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2766 - mae: 0.3625 - val_loss: 0.2971 - val_mae: 0.3795\n",
      "Epoch 22/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2956 - mae: 0.3719 - val_loss: 0.3056 - val_mae: 0.3915\n",
      "Epoch 23/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2870 - mae: 0.3660 - val_loss: 0.2936 - val_mae: 0.3683\n",
      "Epoch 24/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2881 - mae: 0.3613 - val_loss: 0.3082 - val_mae: 0.3903\n",
      "Epoch 25/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2800 - mae: 0.3635 - val_loss: 0.2997 - val_mae: 0.3719\n",
      "Epoch 26/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2680 - mae: 0.3550 - val_loss: 0.2860 - val_mae: 0.3618\n",
      "Epoch 27/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 9ms/step - loss: 0.2787 - mae: 0.3596 - val_loss: 0.2913 - val_mae: 0.3688\n",
      "Epoch 28/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2789 - mae: 0.3625 - val_loss: 0.2892 - val_mae: 0.3729\n",
      "Epoch 29/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2798 - mae: 0.3606 - val_loss: 0.2897 - val_mae: 0.3633\n",
      "Epoch 30/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2728 - mae: 0.3566 - val_loss: 0.2846 - val_mae: 0.3676\n",
      "Epoch 31/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2576 - mae: 0.3486 - val_loss: 0.2877 - val_mae: 0.3638\n",
      "Epoch 32/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2810 - mae: 0.3597 - val_loss: 0.2909 - val_mae: 0.3719\n",
      "Epoch 33/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2749 - mae: 0.3571 - val_loss: 0.2842 - val_mae: 0.3632\n",
      "Epoch 34/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2745 - mae: 0.3576 - val_loss: 0.2921 - val_mae: 0.3758\n",
      "Epoch 35/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.3080 - mae: 0.3569 - val_loss: 0.2837 - val_mae: 0.3715\n",
      "Epoch 36/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 7ms/step - loss: 0.2713 - mae: 0.3552 - val_loss: 0.2911 - val_mae: 0.3729\n",
      "Epoch 37/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 7ms/step - loss: 0.2709 - mae: 0.3567 - val_loss: 0.2851 - val_mae: 0.3640\n",
      "Epoch 38/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 9ms/step - loss: 0.2655 - mae: 0.3551 - val_loss: 0.2895 - val_mae: 0.3773\n",
      "Epoch 39/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 7ms/step - loss: 0.2613 - mae: 0.3500 - val_loss: 0.2782 - val_mae: 0.3630\n",
      "Epoch 40/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2724 - mae: 0.3565 - val_loss: 0.2792 - val_mae: 0.3618\n",
      "Epoch 41/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 8ms/step - loss: 0.2699 - mae: 0.3526 - val_loss: 0.2901 - val_mae: 0.3743\n",
      "Epoch 42/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2521 - mae: 0.3459 - val_loss: 0.2867 - val_mae: 0.3567\n",
      "Epoch 43/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2502 - mae: 0.3441 - val_loss: 0.2813 - val_mae: 0.3634\n",
      "Epoch 44/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 7ms/step - loss: 0.2657 - mae: 0.3519 - val_loss: 0.2779 - val_mae: 0.3629\n",
      "Epoch 45/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.2495 - mae: 0.3441 - val_loss: 0.2837 - val_mae: 0.3742\n",
      "Epoch 46/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.2557 - mae: 0.3457 - val_loss: 0.2765 - val_mae: 0.3573\n",
      "Epoch 47/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 7ms/step - loss: 0.2509 - mae: 0.3418 - val_loss: 0.2852 - val_mae: 0.3652\n",
      "Epoch 48/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 6ms/step - loss: 0.2458 - mae: 0.3410 - val_loss: 0.2826 - val_mae: 0.3557\n",
      "Epoch 49/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2557 - mae: 0.3462 - val_loss: 0.2698 - val_mae: 0.3508\n",
      "Epoch 50/50\n",
      "\u001B[1m516/516\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.2587 - mae: 0.3415 - val_loss: 0.2780 - val_mae: 0.3603\n",
      "\u001B[1m129/129\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - loss: 0.2734 - mae: 0.3598\n",
      "Test Loss: 0.2780, Test MAE: 0.3603\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Corrected MNIST Classification Code",
   "id": "3a3b638fb4d614ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.081057400Z",
     "start_time": "2024-12-17T11:41:42.752168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Enable debug mode for tf.data functions\n",
    "tf.data.experimental.enable_debug_mode()\n",
    "\n",
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input images to [0, 1]\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build the model with explicit Input layer\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),  # Explicit Input layer\n",
    "    Flatten(),  # Flatten 28x28 images\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Softmax for 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ],
   "id": "ecc5aa74fa2ab80c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 13ms/step - accuracy: 0.8720 - loss: 0.4399 - val_accuracy: 0.9623 - val_loss: 0.1234\n",
      "Epoch 2/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 7ms/step - accuracy: 0.9672 - loss: 0.1081 - val_accuracy: 0.9687 - val_loss: 0.0971\n",
      "Epoch 3/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m15s\u001B[0m 8ms/step - accuracy: 0.9788 - loss: 0.0692 - val_accuracy: 0.9726 - val_loss: 0.0852\n",
      "Epoch 4/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m14s\u001B[0m 7ms/step - accuracy: 0.9837 - loss: 0.0505 - val_accuracy: 0.9771 - val_loss: 0.0770\n",
      "Epoch 5/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m13s\u001B[0m 7ms/step - accuracy: 0.9864 - loss: 0.0407 - val_accuracy: 0.9794 - val_loss: 0.0711\n",
      "Epoch 6/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m18s\u001B[0m 10ms/step - accuracy: 0.9898 - loss: 0.0326 - val_accuracy: 0.9765 - val_loss: 0.0770\n",
      "Epoch 7/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9922 - loss: 0.0246 - val_accuracy: 0.9794 - val_loss: 0.0748\n",
      "Epoch 8/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - accuracy: 0.9933 - loss: 0.0194 - val_accuracy: 0.9768 - val_loss: 0.0891\n",
      "Epoch 9/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 5ms/step - accuracy: 0.9940 - loss: 0.0174 - val_accuracy: 0.9781 - val_loss: 0.0851\n",
      "Epoch 10/10\n",
      "\u001B[1m1875/1875\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9953 - loss: 0.0154 - val_accuracy: 0.9816 - val_loss: 0.0780\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step - accuracy: 0.9782 - loss: 0.0932\n",
      "Test Loss: 0.0780, Test Accuracy: 0.9816\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 3 - Learning Iris (binary classification) with Keras",
   "id": "db5cef7b71751dd3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.081057400Z",
     "start_time": "2024-12-17T11:44:58.062791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Loading the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Labels\n",
    "\n",
    "# Converting the problem into a binary classification: \"setosa\" vs \"not setosa\"\n",
    "# Set label 0 for \"setosa\", and 1 for others\n",
    "y_binary = np.where(y == 0, 0, 1)\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizing the feature data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Building the Keras binary classification model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Input layer with feature size\n",
    "    Dense(16, activation='relu'),      # Hidden layer with 16 neurons and ReLU activation\n",
    "    Dense(8, activation='relu'),       # Hidden layer with 8 neurons and ReLU activation\n",
    "    Dense(1, activation='sigmoid')     # Output layer with sigmoid for binary classification\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predicting and displaying sample predictions\n",
    "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "print(\"Predictions (first 10):\", predictions[:10].flatten())\n",
    "print(\"Actual values (first 10):\", y_test[:10])\n"
   ],
   "id": "b0c3cbded20c4504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 39ms/step - accuracy: 0.3667 - loss: 0.6575 - val_accuracy: 0.3333 - val_loss: 0.6796\n",
      "Epoch 2/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.6701 - loss: 0.5801 - val_accuracy: 0.5833 - val_loss: 0.6364\n",
      "Epoch 3/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - accuracy: 0.8039 - loss: 0.5630 - val_accuracy: 0.9167 - val_loss: 0.5972\n",
      "Epoch 4/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.8701 - loss: 0.5195 - val_accuracy: 0.9167 - val_loss: 0.5604\n",
      "Epoch 5/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.8736 - loss: 0.4827 - val_accuracy: 1.0000 - val_loss: 0.5223\n",
      "Epoch 6/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9697 - loss: 0.4529 - val_accuracy: 1.0000 - val_loss: 0.4823\n",
      "Epoch 7/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.9812 - loss: 0.4243 - val_accuracy: 1.0000 - val_loss: 0.4417\n",
      "Epoch 8/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.9721 - loss: 0.3656 - val_accuracy: 1.0000 - val_loss: 0.3997\n",
      "Epoch 9/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.9848 - loss: 0.3116 - val_accuracy: 1.0000 - val_loss: 0.3591\n",
      "Epoch 10/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 1.0000 - loss: 0.2918 - val_accuracy: 1.0000 - val_loss: 0.3190\n",
      "Epoch 11/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.2369 - val_accuracy: 1.0000 - val_loss: 0.2796\n",
      "Epoch 12/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.2423 - val_accuracy: 1.0000 - val_loss: 0.2435\n",
      "Epoch 13/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.1933 - val_accuracy: 1.0000 - val_loss: 0.2112\n",
      "Epoch 14/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 1.0000 - loss: 0.1832 - val_accuracy: 1.0000 - val_loss: 0.1819\n",
      "Epoch 15/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 1.0000 - loss: 0.1307 - val_accuracy: 1.0000 - val_loss: 0.1586\n",
      "Epoch 16/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step - accuracy: 1.0000 - loss: 0.1285 - val_accuracy: 1.0000 - val_loss: 0.1366\n",
      "Epoch 17/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.1216 - val_accuracy: 1.0000 - val_loss: 0.1186\n",
      "Epoch 18/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.1117 - val_accuracy: 1.0000 - val_loss: 0.1027\n",
      "Epoch 19/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0808 - val_accuracy: 1.0000 - val_loss: 0.0900\n",
      "Epoch 20/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0768 - val_accuracy: 1.0000 - val_loss: 0.0788\n",
      "Epoch 21/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0675 - val_accuracy: 1.0000 - val_loss: 0.0689\n",
      "Epoch 22/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0621 - val_accuracy: 1.0000 - val_loss: 0.0609\n",
      "Epoch 23/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 1.0000 - loss: 0.0473 - val_accuracy: 1.0000 - val_loss: 0.0542\n",
      "Epoch 24/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 1.0000 - loss: 0.0523 - val_accuracy: 1.0000 - val_loss: 0.0480\n",
      "Epoch 25/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0435 - val_accuracy: 1.0000 - val_loss: 0.0427\n",
      "Epoch 26/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0371 - val_accuracy: 1.0000 - val_loss: 0.0383\n",
      "Epoch 27/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0331 - val_accuracy: 1.0000 - val_loss: 0.0343\n",
      "Epoch 28/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0329 - val_accuracy: 1.0000 - val_loss: 0.0311\n",
      "Epoch 29/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 1.0000 - loss: 0.0265 - val_accuracy: 1.0000 - val_loss: 0.0282\n",
      "Epoch 30/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step - accuracy: 1.0000 - loss: 0.0289 - val_accuracy: 1.0000 - val_loss: 0.0255\n",
      "Epoch 31/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 1.0000 - loss: 0.0258 - val_accuracy: 1.0000 - val_loss: 0.0233\n",
      "Epoch 32/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0252 - val_accuracy: 1.0000 - val_loss: 0.0213\n",
      "Epoch 33/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0217 - val_accuracy: 1.0000 - val_loss: 0.0196\n",
      "Epoch 34/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0214 - val_accuracy: 1.0000 - val_loss: 0.0179\n",
      "Epoch 35/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0180 - val_accuracy: 1.0000 - val_loss: 0.0166\n",
      "Epoch 36/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 1.0000 - loss: 0.0215 - val_accuracy: 1.0000 - val_loss: 0.0152\n",
      "Epoch 37/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0165 - val_accuracy: 1.0000 - val_loss: 0.0141\n",
      "Epoch 38/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0151 - val_accuracy: 1.0000 - val_loss: 0.0130\n",
      "Epoch 39/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0161 - val_accuracy: 1.0000 - val_loss: 0.0121\n",
      "Epoch 40/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0143 - val_accuracy: 1.0000 - val_loss: 0.0112\n",
      "Epoch 41/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0118 - val_accuracy: 1.0000 - val_loss: 0.0105\n",
      "Epoch 42/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0098\n",
      "Epoch 43/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0133 - val_accuracy: 1.0000 - val_loss: 0.0091\n",
      "Epoch 44/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0107 - val_accuracy: 1.0000 - val_loss: 0.0085\n",
      "Epoch 45/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 1.0000 - loss: 0.0135 - val_accuracy: 1.0000 - val_loss: 0.0080\n",
      "Epoch 46/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0138 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
      "Epoch 47/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0087 - val_accuracy: 1.0000 - val_loss: 0.0071\n",
      "Epoch 48/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0105 - val_accuracy: 1.0000 - val_loss: 0.0067\n",
      "Epoch 49/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 1.0000 - loss: 0.0101 - val_accuracy: 1.0000 - val_loss: 0.0063\n",
      "Epoch 50/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 1.0000 - loss: 0.0089 - val_accuracy: 1.0000 - val_loss: 0.0059\n",
      "Test Loss: 0.0061\n",
      "Test Accuracy: 1.0000\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 85ms/step\n",
      "Predictions (first 10): [1 0 1 1 1 0 1 1 1 1]\n",
      "Actual values (first 10): [1 0 1 1 1 0 1 1 1 1]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 4 - Learning Iris (multi-value classification) with Keras",
   "id": "ee3236bbb6d9bbbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.081057400Z",
     "start_time": "2024-12-17T11:45:29.338525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Loading the Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data  # Features\n",
    "y = data.target  # Labels: 0 (setosa), 1 (versicolor), 2 (virginica)\n",
    "\n",
    "# Converting labels to categorical (one-hot encoding)\n",
    "y_categorical = to_categorical(y, num_classes=3)\n",
    "\n",
    "# Splitting data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalizing the feature data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Building the Keras ternary classification model\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Input layer with feature size\n",
    "    Dense(16, activation='relu'),      # Hidden layer with 16 neurons and ReLU activation\n",
    "    Dense(8, activation='relu'),       # Hidden layer with 8 neurons and ReLU activation\n",
    "    Dense(3, activation='softmax')     # Output layer with softmax for ternary classification\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',  # Loss for multi-class classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=8, validation_split=0.1, verbose=1)\n",
    "\n",
    "# Evaluating the model on test data\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Predicting and displaying sample predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = predictions.argmax(axis=1)\n",
    "actual_classes = y_test.argmax(axis=1)\n",
    "\n",
    "print(\"Predicted Classes (first 10):\", predicted_classes[:10])\n",
    "print(\"Actual Classes (first 10):\", actual_classes[:10])\n"
   ],
   "id": "a883f6f5a840d331",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 33ms/step - accuracy: 0.4053 - loss: 1.0832 - val_accuracy: 0.5000 - val_loss: 1.0659\n",
      "Epoch 2/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.3718 - loss: 1.0536 - val_accuracy: 0.5000 - val_loss: 1.0412\n",
      "Epoch 3/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.5952 - loss: 1.0180 - val_accuracy: 0.5000 - val_loss: 1.0159\n",
      "Epoch 4/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.6202 - loss: 1.0006 - val_accuracy: 0.5000 - val_loss: 0.9930\n",
      "Epoch 5/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7064 - loss: 0.9581 - val_accuracy: 0.5000 - val_loss: 0.9655\n",
      "Epoch 6/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.7745 - loss: 0.8949 - val_accuracy: 0.7500 - val_loss: 0.9346\n",
      "Epoch 7/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.8445 - loss: 0.8592 - val_accuracy: 0.7500 - val_loss: 0.9040\n",
      "Epoch 8/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8594 - loss: 0.8298 - val_accuracy: 0.8333 - val_loss: 0.8700\n",
      "Epoch 9/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.8126 - loss: 0.7479 - val_accuracy: 0.8333 - val_loss: 0.8342\n",
      "Epoch 10/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.8221 - loss: 0.6987 - val_accuracy: 0.8333 - val_loss: 0.7981\n",
      "Epoch 11/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.8351 - loss: 0.6820 - val_accuracy: 0.8333 - val_loss: 0.7665\n",
      "Epoch 12/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.7977 - loss: 0.6793 - val_accuracy: 0.8333 - val_loss: 0.7349\n",
      "Epoch 13/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8223 - loss: 0.5733 - val_accuracy: 0.9167 - val_loss: 0.6994\n",
      "Epoch 14/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.8521 - loss: 0.5238 - val_accuracy: 0.9167 - val_loss: 0.6705\n",
      "Epoch 15/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.8761 - loss: 0.4717 - val_accuracy: 0.9167 - val_loss: 0.6399\n",
      "Epoch 16/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9113 - loss: 0.4546 - val_accuracy: 0.9167 - val_loss: 0.6068\n",
      "Epoch 17/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.8883 - loss: 0.4372 - val_accuracy: 0.9167 - val_loss: 0.5730\n",
      "Epoch 18/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9026 - loss: 0.4327 - val_accuracy: 0.9167 - val_loss: 0.5359\n",
      "Epoch 19/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9210 - loss: 0.3830 - val_accuracy: 0.9167 - val_loss: 0.5000\n",
      "Epoch 20/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9601 - loss: 0.3486 - val_accuracy: 0.9167 - val_loss: 0.4739\n",
      "Epoch 21/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9493 - loss: 0.3356 - val_accuracy: 0.9167 - val_loss: 0.4487\n",
      "Epoch 22/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9698 - loss: 0.2904 - val_accuracy: 0.9167 - val_loss: 0.4250\n",
      "Epoch 23/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9744 - loss: 0.2870 - val_accuracy: 0.9167 - val_loss: 0.4149\n",
      "Epoch 24/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9526 - loss: 0.2606 - val_accuracy: 0.9167 - val_loss: 0.4055\n",
      "Epoch 25/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9547 - loss: 0.2248 - val_accuracy: 0.9167 - val_loss: 0.3945\n",
      "Epoch 26/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9771 - loss: 0.2172 - val_accuracy: 0.9167 - val_loss: 0.3901\n",
      "Epoch 27/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - accuracy: 0.9758 - loss: 0.1893 - val_accuracy: 0.9167 - val_loss: 0.3857\n",
      "Epoch 28/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.9722 - loss: 0.1688 - val_accuracy: 0.9167 - val_loss: 0.3828\n",
      "Epoch 29/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9408 - loss: 0.2056 - val_accuracy: 0.9167 - val_loss: 0.3751\n",
      "Epoch 30/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9504 - loss: 0.1864 - val_accuracy: 0.9167 - val_loss: 0.3764\n",
      "Epoch 31/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.9705 - loss: 0.1671 - val_accuracy: 0.9167 - val_loss: 0.3771\n",
      "Epoch 32/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - accuracy: 0.9607 - loss: 0.1719 - val_accuracy: 0.9167 - val_loss: 0.3750\n",
      "Epoch 33/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9548 - loss: 0.1457 - val_accuracy: 0.9167 - val_loss: 0.3753\n",
      "Epoch 34/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9342 - loss: 0.1828 - val_accuracy: 0.9167 - val_loss: 0.3819\n",
      "Epoch 35/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9676 - loss: 0.1603 - val_accuracy: 0.9167 - val_loss: 0.3790\n",
      "Epoch 36/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9520 - loss: 0.1350 - val_accuracy: 0.9167 - val_loss: 0.3873\n",
      "Epoch 37/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.9358 - loss: 0.1543 - val_accuracy: 0.9167 - val_loss: 0.3828\n",
      "Epoch 38/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9450 - loss: 0.1436 - val_accuracy: 0.9167 - val_loss: 0.3913\n",
      "Epoch 39/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9754 - loss: 0.1359 - val_accuracy: 0.9167 - val_loss: 0.3931\n",
      "Epoch 40/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9767 - loss: 0.0975 - val_accuracy: 0.9167 - val_loss: 0.4019\n",
      "Epoch 41/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9793 - loss: 0.0999 - val_accuracy: 0.9167 - val_loss: 0.3947\n",
      "Epoch 42/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9619 - loss: 0.1045 - val_accuracy: 0.9167 - val_loss: 0.4103\n",
      "Epoch 43/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9678 - loss: 0.1163 - val_accuracy: 0.9167 - val_loss: 0.4053\n",
      "Epoch 44/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - accuracy: 0.9861 - loss: 0.0831 - val_accuracy: 0.9167 - val_loss: 0.4070\n",
      "Epoch 45/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9899 - loss: 0.0754 - val_accuracy: 0.9167 - val_loss: 0.4092\n",
      "Epoch 46/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9857 - loss: 0.0805 - val_accuracy: 0.9167 - val_loss: 0.4172\n",
      "Epoch 47/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9630 - loss: 0.1133 - val_accuracy: 0.9167 - val_loss: 0.4110\n",
      "Epoch 48/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9658 - loss: 0.0959 - val_accuracy: 0.9167 - val_loss: 0.4190\n",
      "Epoch 49/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - accuracy: 0.9630 - loss: 0.1120 - val_accuracy: 0.9167 - val_loss: 0.4219\n",
      "Epoch 50/50\n",
      "\u001B[1m14/14\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - accuracy: 0.9789 - loss: 0.0942 - val_accuracy: 0.9167 - val_loss: 0.4139\n",
      "Test Loss: 0.0776\n",
      "Test Accuracy: 1.0000\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 100ms/step\n",
      "Predicted Classes (first 10): [1 0 2 1 1 0 1 2 1 1]\n",
      "Actual Classes (first 10): [1 0 2 1 1 0 1 2 1 1]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 5 - Learning House Prices with Keras",
   "id": "53e20cafad8c09c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.081057400Z",
     "start_time": "2024-12-17T12:04:22.494207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.random.rand(1000, 10) * 1000  # Replace with your actual dataset\n",
    "y = np.random.rand(1000, 1) * 1e6   # Large target values\n",
    "\n",
    "# Preprocessing: Normalize input and target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model definition\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1],)),  # Input shape matches number of features\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Evaluate\n",
    "loss, mae = model.evaluate(X_val, y_val)\n",
    "print(\"Validation Loss:\", loss)\n",
    "print(\"Validation MAE:\", mae)\n",
    "\n",
    "# Rescale predictions\n",
    "predictions = scaler_y.inverse_transform(model.predict(X_val))\n"
   ],
   "id": "58472c4023e5f101",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 18ms/step - loss: 1.1394 - mae: 0.9021 - val_loss: 0.9732 - val_mae: 0.8485\n",
      "Epoch 2/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 1.1108 - mae: 0.9129 - val_loss: 0.9663 - val_mae: 0.8391\n",
      "Epoch 3/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 1.0127 - mae: 0.8591 - val_loss: 0.9668 - val_mae: 0.8410\n",
      "Epoch 4/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 1.0008 - mae: 0.8639 - val_loss: 0.9601 - val_mae: 0.8371\n",
      "Epoch 5/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.9437 - mae: 0.8321 - val_loss: 0.9574 - val_mae: 0.8333\n",
      "Epoch 6/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.9140 - mae: 0.8192 - val_loss: 0.9479 - val_mae: 0.8311\n",
      "Epoch 7/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.9432 - mae: 0.8394 - val_loss: 0.9610 - val_mae: 0.8329\n",
      "Epoch 8/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.9229 - mae: 0.8260 - val_loss: 0.9473 - val_mae: 0.8291\n",
      "Epoch 9/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8834 - mae: 0.8063 - val_loss: 0.9615 - val_mae: 0.8275\n",
      "Epoch 10/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8873 - mae: 0.7967 - val_loss: 0.9529 - val_mae: 0.8325\n",
      "Epoch 11/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.9113 - mae: 0.8243 - val_loss: 0.9646 - val_mae: 0.8283\n",
      "Epoch 12/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8686 - mae: 0.7951 - val_loss: 0.9533 - val_mae: 0.8231\n",
      "Epoch 13/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8928 - mae: 0.8049 - val_loss: 0.9563 - val_mae: 0.8248\n",
      "Epoch 14/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8527 - mae: 0.7831 - val_loss: 0.9518 - val_mae: 0.8223\n",
      "Epoch 15/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8692 - mae: 0.8028 - val_loss: 0.9569 - val_mae: 0.8236\n",
      "Epoch 16/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8724 - mae: 0.7904 - val_loss: 0.9523 - val_mae: 0.8205\n",
      "Epoch 17/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8158 - mae: 0.7639 - val_loss: 0.9586 - val_mae: 0.8241\n",
      "Epoch 18/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.8150 - mae: 0.7755 - val_loss: 0.9570 - val_mae: 0.8216\n",
      "Epoch 19/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.8354 - mae: 0.7850 - val_loss: 0.9605 - val_mae: 0.8216\n",
      "Epoch 20/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.8027 - mae: 0.7579 - val_loss: 0.9589 - val_mae: 0.8204\n",
      "Epoch 21/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.7686 - mae: 0.7459 - val_loss: 0.9608 - val_mae: 0.8215\n",
      "Epoch 22/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.7419 - mae: 0.7161 - val_loss: 0.9650 - val_mae: 0.8221\n",
      "Epoch 23/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.7640 - mae: 0.7348 - val_loss: 0.9646 - val_mae: 0.8224\n",
      "Epoch 24/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.7669 - mae: 0.7348 - val_loss: 0.9650 - val_mae: 0.8222\n",
      "Epoch 25/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.7446 - mae: 0.7315 - val_loss: 0.9728 - val_mae: 0.8238\n",
      "Epoch 26/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.7526 - mae: 0.7353 - val_loss: 0.9706 - val_mae: 0.8227\n",
      "Epoch 27/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.7388 - mae: 0.7206 - val_loss: 0.9703 - val_mae: 0.8234\n",
      "Epoch 28/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.7420 - mae: 0.7372 - val_loss: 0.9905 - val_mae: 0.8279\n",
      "Epoch 29/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.7621 - mae: 0.7393 - val_loss: 0.9927 - val_mae: 0.8280\n",
      "Epoch 30/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.6645 - mae: 0.6759 - val_loss: 0.9838 - val_mae: 0.8258\n",
      "Epoch 31/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.6894 - mae: 0.6982 - val_loss: 0.9919 - val_mae: 0.8270\n",
      "Epoch 32/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.6681 - mae: 0.6915 - val_loss: 1.0209 - val_mae: 0.8360\n",
      "Epoch 33/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.6834 - mae: 0.6823 - val_loss: 1.0145 - val_mae: 0.8367\n",
      "Epoch 34/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5838 - mae: 0.6306 - val_loss: 1.0014 - val_mae: 0.8303\n",
      "Epoch 35/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.6224 - mae: 0.6600 - val_loss: 1.0324 - val_mae: 0.8402\n",
      "Epoch 36/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.6063 - mae: 0.6392 - val_loss: 1.0180 - val_mae: 0.8356\n",
      "Epoch 37/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.6322 - mae: 0.6644 - val_loss: 1.0349 - val_mae: 0.8370\n",
      "Epoch 38/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.5627 - mae: 0.6247 - val_loss: 1.0336 - val_mae: 0.8398\n",
      "Epoch 39/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6197 - mae: 0.6520 - val_loss: 1.0508 - val_mae: 0.8460\n",
      "Epoch 40/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.6438 - mae: 0.6665 - val_loss: 1.0706 - val_mae: 0.8504\n",
      "Epoch 41/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5626 - mae: 0.6144 - val_loss: 1.0325 - val_mae: 0.8402\n",
      "Epoch 42/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5761 - mae: 0.6294 - val_loss: 1.0405 - val_mae: 0.8464\n",
      "Epoch 43/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5728 - mae: 0.6207 - val_loss: 1.0553 - val_mae: 0.8476\n",
      "Epoch 44/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5741 - mae: 0.6250 - val_loss: 1.0776 - val_mae: 0.8558\n",
      "Epoch 45/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5326 - mae: 0.5964 - val_loss: 1.0767 - val_mae: 0.8556\n",
      "Epoch 46/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5309 - mae: 0.5963 - val_loss: 1.0702 - val_mae: 0.8543\n",
      "Epoch 47/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.5263 - mae: 0.6005 - val_loss: 1.0833 - val_mae: 0.8521\n",
      "Epoch 48/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.5059 - mae: 0.5832 - val_loss: 1.0965 - val_mae: 0.8640\n",
      "Epoch 49/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.5049 - mae: 0.5935 - val_loss: 1.1638 - val_mae: 0.8891\n",
      "Epoch 50/50\n",
      "\u001B[1m25/25\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 8ms/step - loss: 0.5035 - mae: 0.5783 - val_loss: 1.1129 - val_mae: 0.8636\n",
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 1.0589 - mae: 0.8393 \n",
      "Validation Loss: 1.1128562688827515\n",
      "Validation MAE: 0.8636088371276855\n",
      "\u001B[1m7/7\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Problem 6 - Learning MNIST with Keras",
   "id": "63f2d54c2e845836"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T12:38:40.081057400Z",
     "start_time": "2024-12-17T12:12:09.748958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Printing dataset shape\n",
    "print(f\"Training Data Shape: {X_train.shape}, Training Labels Shape: {y_train.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}, Test Labels Shape: {y_test.shape}\")\n",
    "\n",
    "# Preprocessing the data\n",
    "X_train = X_train / 255.0  # Normalize pixel values to the range [0, 1]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding for multi-class classification\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Building the Keras model\n",
    "model = Sequential([\n",
    "    Input(shape=(28, 28)),  # Explicit Input layer\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,  # Reserve 10% of training data for validation\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    verbose=1)\n",
    "\n",
    "# Evaluating the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Making predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Plot first 5 test images, predictions, and true labels\n",
    "for i in range(5):\n",
    "    plt.imshow(X_test[i], cmap='gray')\n",
    "    plt.title(f\"True: {np.argmax(y_test[i])}, Predicted: {np.argmax(predictions[i])}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ],
   "id": "1aa6abf27e9325a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (60000, 28, 28), Training Labels Shape: (60000,)\n",
      "Test Data Shape: (10000, 28, 28), Test Labels Shape: (10000,)\n",
      "Epoch 1/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 6ms/step - accuracy: 0.8711 - loss: 0.4449 - val_accuracy: 0.9638 - val_loss: 0.1342\n",
      "Epoch 2/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m16s\u001B[0m 10ms/step - accuracy: 0.9608 - loss: 0.1339 - val_accuracy: 0.9707 - val_loss: 0.1017\n",
      "Epoch 3/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - accuracy: 0.9754 - loss: 0.0841 - val_accuracy: 0.9728 - val_loss: 0.0981\n",
      "Epoch 4/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 6ms/step - accuracy: 0.9806 - loss: 0.0635 - val_accuracy: 0.9780 - val_loss: 0.0763\n",
      "Epoch 5/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9865 - loss: 0.0460 - val_accuracy: 0.9773 - val_loss: 0.0765\n",
      "Epoch 6/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - accuracy: 0.9881 - loss: 0.0377 - val_accuracy: 0.9772 - val_loss: 0.0780\n",
      "Epoch 7/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9911 - loss: 0.0288 - val_accuracy: 0.9813 - val_loss: 0.0703\n",
      "Epoch 8/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 6ms/step - accuracy: 0.9932 - loss: 0.0235 - val_accuracy: 0.9788 - val_loss: 0.0826\n",
      "Epoch 9/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 6ms/step - accuracy: 0.9939 - loss: 0.0209 - val_accuracy: 0.9798 - val_loss: 0.0785\n",
      "Epoch 10/10\n",
      "\u001B[1m1688/1688\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 7ms/step - accuracy: 0.9957 - loss: 0.0150 - val_accuracy: 0.9802 - val_loss: 0.0877\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 5ms/step - accuracy: 0.9769 - loss: 0.0917\n",
      "\n",
      "Test Accuracy: 0.9790\n",
      "Test Loss: 0.0781\n",
      "\u001B[1m313/313\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problem 7 - (Advanced assignment) Rewriting to PyTorch\n",
    "\n",
    "Multi-Class Classification: Iris Dataset"
   ],
   "id": "f81a22d2dc6147bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T13:13:05.170522Z",
     "start_time": "2024-12-17T13:13:02.111666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = load_iris()\n",
    "X = torch.tensor(data.data, dtype=torch.float32)\n",
    "y = torch.tensor(data.target, dtype=torch.long)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "class IrisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 16),  # Input size = 4 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 3)   # Output size = 3 classes\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = IrisModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    predictions = torch.argmax(test_outputs, axis=1)\n",
    "    accuracy = (predictions == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ],
   "id": "95013d3d82fd90bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Binary Classification: Iris Dataset",
   "id": "d4dfabfa568f875c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T13:13:10.923131Z",
     "start_time": "2024-12-17T13:13:10.421178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = load_iris()\n",
    "X = torch.tensor(data.data, dtype=torch.float32)\n",
    "y = torch.tensor((data.target == 0).astype(int), dtype=torch.float32)  # Binary: Setosa (1) vs Others (0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "class BinaryIrisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryIrisModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4, 16),  # Input size = 4 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)   # Output size = 1 (binary)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = BinaryIrisModel()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train).squeeze()\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    test_outputs = torch.sigmoid(model(X_test).squeeze())\n",
    "    predictions = (test_outputs > 0.5).float()\n",
    "    accuracy = (predictions == y_test).sum().item() / y_test.size(0)\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}\")\n"
   ],
   "id": "67fc30ec7ea99644",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Regression Problem: Housing Prices Dataset",
   "id": "9ae6bf37bf7e00a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T13:13:17.447472Z",
     "start_time": "2024-12-17T13:13:16.505013Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data = fetch_california_housing()\n",
    "scaler = StandardScaler()\n",
    "X = torch.tensor(scaler.fit_transform(data.data), dtype=torch.float32)\n",
    "y = torch.tensor(data.target, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "class HousingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HousingModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8, 32),  # Input size = 8 features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)   # Output size = 1 continuous value\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = HousingModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    mse = criterion(test_outputs, y_test)\n",
    "    print(f\"Test MSE: {mse.item():.4f}\")\n"
   ],
   "id": "96086d1e867f9c17",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.4451\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Image Classification: MNIST Dataset",
   "id": "4eae37247860e7aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T13:16:51.992635Z",
     "start_time": "2024-12-17T13:13:22.625124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 14 * 14)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "model = CNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(5):\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ],
   "id": "c19a1a6f10911a99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:06<00:00, 1.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 147kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.15MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Test Accuracy: 98.38%\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Problem 8 - (Advanced assignment) Comparing frameworks\n",
    "\n",
    "PyTorch\n",
    "\n",
    "PyTorch is widely regarded as the most flexible and Pythonic deep learning framework. It is known for its dynamic computation graphs, which allow for flexible model building and debugging. PyTorch is fast and works well with GPUs, making it a popular choice for researchers. The code written in PyTorch is clean, concise, and closely follows Python's syntax, making it very readable. This framework is particularly suited for complex operations, custom models, and experimental research tasks like natural language processing (NLP) and computer vision (CV).\n",
    "\n",
    "TensorFlow\n",
    "\n",
    "TensorFlow, developed by Google, focuses on production readiness and scalability. It utilizes static computation graphs, which allow for efficient execution and deployment of models. TensorFlow's performance is highly optimized for large-scale training and inference. While TensorFlow offers many advanced tools like TensorBoard for visualization, TensorFlow Lite for mobile deployment, and TensorFlow Serving for serving models, the code can feel more verbose and less intuitive compared to PyTorch.\n",
    "\n",
    "\n",
    "Keras\n",
    "\n",
    "Keras is a high-level API that simplifies deep learning workflows. It runs on top of TensorFlow and focuses on ease of use, readability, and rapid prototyping. Keras makes it possible to write deep learning models with just a few lines of code, making it perfect for beginners or anyone working on standard tasks like classification and regression. While Keras abstracts much of the complexity, it may lack the fine-grained control that frameworks like PyTorch offer. Performance-wise, it relies on the TensorFlow backend, so it can be slightly slower than native PyTorch or TensorFlow."
   ],
   "id": "f9ba2fab94ff4dde"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
