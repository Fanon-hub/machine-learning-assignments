{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btsnrObeUIzl"
   },
   "source": "Problem 1 Implementation of Forward Propagation for SimpleRNN"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F9pE_J7GUDdL",
    "ExecuteTime": {
     "end_time": "2024-12-20T07:32:57.230824Z",
     "start_time": "2024-12-20T07:32:57.004643Z"
    }
   },
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jp4YpCndOmTq",
    "ExecuteTime": {
     "end_time": "2024-12-20T07:32:57.253971Z",
     "start_time": "2024-12-20T07:32:57.234945Z"
    }
   },
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JT87qafsULpX",
    "outputId": "740878aa-6782-4c44-a7a1-349464af61f7",
    "ExecuteTime": {
     "end_time": "2024-12-20T07:32:58.072344Z",
     "start_time": "2024-12-20T07:32:58.041390Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "#Problem 1 - Forward propagation implementation of SimpleRNN\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "\n",
    "rnn = SimpleRNN(w_x, w_h, b)\n",
    "h_list = []\n",
    "h_list.append(h)\n",
    "\n",
    "for i in range(n_sequences):\n",
    "    h = rnn.forward(x[:, i, :], h)\n",
    "    h_list.append(h)\n",
    "\n",
    "h_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0.]]),\n",
       " array([[0.76188798, 0.76213958, 0.76239095, 0.76255841]]),\n",
       " array([[0.792209  , 0.8141834 , 0.83404912, 0.84977719]]),\n",
       " array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyicL8tLU87x"
   },
   "source": "Problem 2 - Experimenting with Forward Propagation on a Small Array"
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-8bc1bIuVct2",
    "outputId": "c55f4bb8-c5a6-430d-be74-16fc1fd7a194",
    "ExecuteTime": {
     "end_time": "2024-12-20T07:32:58.142963Z",
     "start_time": "2024-12-20T07:32:58.113703Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleRNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "\n",
    "rnn = SimpleRNN(w_x, w_h, b)\n",
    "h_list = []\n",
    "h_list.append(h)\n",
    "\n",
    "for i in range(n_sequences):\n",
    "    h = rnn.forward(x[:, i, :], h)\n",
    "    h_list.append(h)\n",
    "\n",
    "h_list"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., 0.]]),\n",
       " array([[0.76188798, 0.76213958, 0.76239095, 0.76255841]]),\n",
       " array([[0.792209  , 0.8141834 , 0.83404912, 0.84977719]]),\n",
       " array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmKTpDylVjXb"
   },
   "source": "Problem 3 - Implementation of Backward Propagation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HMceXmm2Vn6r",
    "ExecuteTime": {
     "end_time": "2024-12-20T07:32:58.261681Z",
     "start_time": "2024-12-20T07:32:58.243603Z"
    }
   },
   "source": [
    "class SimpleRNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "\n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        return h_next\n",
    "\n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "\n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "\n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "\n",
    "        return dx, dh_prev"
   ],
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNHxlc4vuCANOMKNdhsgTvA",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
